# -*- coding: utf-8 -*-
"""Paraphrasing-Mehtod3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ayVBTKVAx6paur5wJEvyWCfJ5dRg8rdc
"""

## import libraries 
from keras.layers import Dense,LSTM,Embedding,GRU,Input,Dropout,RepeatVector,Flatten
from keras.models import Sequential,Model
from keras.preprocessing.text import Tokenizer
import numpy as np
import keras.utils as KU
import traceback
import numpy as np
from keras.preprocessing.sequence import pad_sequences
from keras import optimizers
import tensorflow as tf
import keras.losses as KL
from keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard

num_words=1000

##read dataset
with open('quora_duplicate_questions.tsv',encoding="utf-8") as f:
  data=f.read().split('\n')

corpus=[]
for i in data:
  line=i.split('\t')
  if(i!= " " and len(line)>5):
    is_duplicate=line[5]
    if(is_duplicate=='1'):
        corpus.append(i)

input_texts=[]
target_texts=[]
input_characters=set()
target_characters=set()

## Not create input text as English and output text as Hindi
try:
    for line in corpus[:1000]:
        ##print(line)
        if(line !=""):
            text=line.split('\t')
            input_text=text[3]
            target_text=text[4]
            input_texts.append(input_text.lower())
            target_text='ssss '+target_text.lower()+' eeee'
            ##print(target_text)
            target_texts.append(target_text)
            for char in input_text:
                if char not in input_characters:
                    input_characters.add(char)
                
            for char in target_text:
                if char not in target_characters:
                    target_characters.add(char)
except Exception as e:
    print(e)
    traceback.print_exc()

max_sequence_len_input = max([len(x) for x in input_texts])
max_sequence_len_target = max([len(x) for x in target_texts])

input_tokenizer=Tokenizer(filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n')
input_tokenizer.fit_on_texts(input_texts)

target_tokenizer=Tokenizer(filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n')
target_tokenizer.fit_on_texts(target_texts)

input_vocab_size=len(input_tokenizer.word_index)+1
target_vocab_size=len(target_tokenizer.word_index)+1

## Now Generate the embeddings from Glove vector
embeddings_index={}
def Generate_Embeddings():
    f=open('../content/glove.6B.100d.txt') 
    for line in f:
        #print(line)
        values=line.split()
        word=values[0]
        coefs=np.array(values[1:],dtype="float32")
        embeddings_index[word]=coefs
    f.close()

## Now Generate Metrix of embedding as per our data
embedding_matrix = np.zeros((input_vocab_size, 100))
def Generate_Embeddings_Matrix():
    for word, i in input_tokenizer.word_index.items():
        #print(word,":::",i)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

Generate_Embeddings()

# encode and pad sequences
def encode_sequences(tokenizer, length, lines):
    # integer encode sequences
    seq = tokenizer.texts_to_sequences(lines)
    # pad sequences with 0 values
    seq = pad_sequences(seq, maxlen=length, padding='post')
    return seq

encoder_input_data=encode_sequences(input_tokenizer,max_sequence_len_input,input_texts)
decoder_input_data=encode_sequences(target_tokenizer,max_sequence_len_target,target_texts)
decoder_input_data_final=decoder_input_data[:,:-1]
decoder_target_data=decoder_input_data[:,1:]

## create an encoder archetecture
encoder_input=Input(shape=(None,))
#encoder_embeddings=Embedding(input_vocab_size, 100, input_length=max_sequence_len_input-1, mask_zero=True,trainable=False)
encoder_embeddings=Embedding(input_dim=input_vocab_size,output_dim=128,name='encoder_embedding')
encoder_gru_1=GRU(512,name="encoder_gru_1",return_sequences=True)
encoder_gru_2=GRU(512,name="encoder_gru_2",return_sequences=True)
encoder_gru_3=GRU(512,name="encoder_gru_3",return_sequences=False)

def Create_Encoder():
    #Here we will use functional api of keras
    net=encoder_input
    net=encoder_embeddings(net)
    ## not need to connect GRU units
    net=encoder_gru_1(net)
    net=encoder_gru_2(net)
    net=encoder_gru_3(net)
    
    return net

##Now create decoder architecture
decoder_initial_state=Input(shape=(512,),name="decoder_initial_state")
decoder_input=Input(shape=(None,),name="decoder_input")
#decoder_embeddings=Embedding(target_vocab_size, 100, input_length=max_sequence_len_target-1,trainable=False)
decoder_embeddings=Embedding(input_dim=target_vocab_size,output_dim=128,name='decoder_embedding')
decoder_gru_1=GRU(512,name="decoder_gru_1",return_sequences=True)
decoder_gru_2=GRU(512,name="decoder_gru_2",return_sequences=True)
decoder_gru_3=GRU(512,name="decoder_gru_3",return_sequences=True)
decoder_flatten=Flatten()
decoder_dense=Dense(target_vocab_size,activation="linear")

def Create_Decoder(initial_state):
    #Here we will use functional api of keras
    net=decoder_input
    net=decoder_embeddings(net)
    ## not need to connect GRU units
    net=decoder_gru_1(net,initial_state=initial_state)
    net=decoder_gru_2(net,initial_state=initial_state)
    net=decoder_gru_3(net,initial_state=initial_state)
    #net=decoder_flatten(net)
    net=decoder_dense(net)
    
    return net

##Create a Training Model
encoder_output=Create_Encoder()
decoder_output=Create_Decoder(encoder_output)
model_train=Model(inputs=[encoder_input,decoder_input],outputs=[decoder_output])

## create an Encoder Model
model_encoder=Model(inputs=[encoder_input],outputs=[encoder_output])

## create a decoder model
decoder_output=Create_Decoder(decoder_initial_state)
model_decoder=Model(inputs=[decoder_input,decoder_initial_state],outputs=[decoder_output])

optimizer=optimizers.RMSprop(lr=1e-3)

def sparse_cross_entropy(y_true, y_pred):
    """
    Calculate the cross-entropy loss between y_true and y_pred.
    
    y_true is a 2-rank tensor with the desired output.
    The shape is [batch_size, sequence_length] and it
    contains sequences of integer-tokens.

    y_pred is the decoder's output which is a 3-rank tensor
    with shape [batch_size, sequence_length, num_words]
    so that for each sequence in the batch there is a one-hot
    encoded array of length num_words.
    """

    # Calculate the loss. This outputs a
    # 2-rank tensor of shape [batch_size, sequence_length]
    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred)

    # Keras may reduce this across the first axis (the batch)
    # but the semantics are unclear, so to be sure we use
    # the loss across the entire 2-rank tensor, we reduce it
    # to a single scalar with the mean function.
    loss_mean = tf.reduce_mean(loss)

    return loss_mean

decoder_target = tf.placeholder(dtype='float32', shape=(None, None))

model_train.compile(optimizer=optimizer, loss=KL.sparse_categorical_crossentropy,metrics=['acc'],target_tensors=[decoder_target])
model_train.summary()

path_checkpoint='21_checkpoint.keras'
callback_checkpoint=ModelCheckpoint(filepath=path_checkpoint,monitor='val_loss',verbose=1, save_best_only=True, save_weights_only=True)
callback_earlystopping=EarlyStopping(monitor='val_loss',verbose=1,patience=3)
callback_tensorboard=TensorBoard(log_dir='../content/21_logs/',histogram_freq=0,write_graph=False)
callbacks=[callback_checkpoint,callback_earlystopping,callback_tensorboard]

x_data={
    'encoder-input':encoder_input_data,
    'decoder-input':decoder_input_data
}
y_data={
    'decoder_output':decoder_target_data
}

model_train.fit([encoder_input_data, decoder_input_data],decoder_target_data.reshape(decoder_target_data.shape[0], decoder_target_data.shape[1], 1),batch_size=640,epochs=20,validation_split=0.3,callbacks=callbacks)

model_train.save('ModelNew.h5')
model_encoder.save('ModelEncoder.h5')
model_decoder.save('ModelDecoder.h5')


token_start = input_tokenizer.word_index['ssss']
token_end = target_tokenizer.word_index['eeee']

def generate(input_text, true_output_text=None):
    """Translate a single text-string."""

    # Convert the input-text to integer-tokens.
    # Note the sequence of tokens has to be reversed.
    # Padding is probably not necessary.
    input_tokens = input_tokenizer.texts_to_sequences(input_text)
    
    # Get the output of the encoder's GRU which will be
    # used as the initial state in the decoder's GRU.
    # This could also have been the encoder's final state
    # but that is really only necessary if the encoder
    # and decoder use the LSTM instead of GRU because
    # the LSTM has two internal states.
    initial_state = model_encoder.predict(input_tokens)

    # Max number of tokens / words in the output sequence.
    max_tokens =max_sequence_len_target

    # Pre-allocate the 2-dim array used as input to the decoder.
    # This holds just a single sequence of integer-tokens,
    # but the decoder-model expects a batch of sequences.
    shape = (1, max_tokens)
    decoder_input_data = np.zeros(shape=shape, dtype=np.int)

    # The first input-token is the special start-token for 'ssss '.
    token_int = token_start

    # Initialize an empty output-text.
    output_text = ''

    # Initialize the number of tokens we have processed.
    count_tokens = 0

    # While we haven't sampled the special end-token for ' eeee'
    # and we haven't processed the max number of tokens.
    while token_int != token_end and count_tokens < max_tokens:
        # Update the input-sequence to the decoder
        # with the last token that was sampled.
        # In the first iteration this will set the
        # first element to the start-token.
        decoder_input_data[0, count_tokens] = token_int

        # Wrap the input-data in a dict for clarity and safety,
        # so we are sure we input the data in the right order.
        x_data = \
        {
            'decoder_initial_state': initial_state,
            'decoder_input': decoder_input_data
        }

        # Note that we input the entire sequence of tokens
        # to the decoder. This wastes a lot of computation
        # because we are only interested in the last input
        # and output. We could modify the code to return
        # the GRU-states when calling predict() and then
        # feeding these GRU-states as well the next time
        # we call predict(), but it would make the code
        # much more complicated.

        # Input this data to the decoder and get the predicted output.
        decoder_output = model_decoder.predict(x_data)

        # Get the last predicted token as a one-hot encoded array.
        token_onehot = decoder_output[0, count_tokens, :]
        
        # Convert to an integer-token.
        token_int = np.argmax(token_onehot)

        # Lookup the word corresponding to this integer-token.
        sampled_word = target_tokenizer.sequences_to_texts(token_int)

        # Append the word to the output-text.
        output_text += " " + sampled_word

        # Increment the token-counter.
        count_tokens += 1

    # Sequence of tokens output by the decoder.
    output_tokens = decoder_input_data[0]
    
    # Print the input-text.
    print("Input text:")
    print(input_text)
    print()

    # Print the translated output-text.
    print("Translated text:")
    print(output_text)
    print()

    # Optionally print the true translated text.
    if true_output_text is not None:
        print("True output text:")
        print(true_output_text)
        print()


generate(input_text=input_texts[2],true_output_text=target_texts[2])
generate(input_text=input_texts[20],true_output_text=target_texts[20])
generate(input_text=input_texts[22],true_output_text=target_texts[22])
generate(input_text=input_texts[100],true_output_text=target_texts[100])

