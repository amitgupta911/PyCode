# -*- coding: utf-8 -*-
"""SimpleParaphrasing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C7-9kdkHQiTyS-cg1NajPN8dTV9GWyCj
"""

## import libraries 
from keras.layers import Dense,LSTM,Embedding,GRU,Input
from keras.models import Sequential,Model
from keras.preprocessing.text import Tokenizer
import numpy as np
import keras.utils as KU
import traceback
import numpy as np
from keras.models import load_mode


input_texts=[]
target_texts=[]
input_characters=set()
target_characters=set()

## Not create input text as English and output text as Hindi
try:
  for line in corpus[:10000]:
      ##print(line)
      if(line !=""):
          text=line.split('\t')
          input_text=text[3]
          target_text=text[4]
          input_texts.append(input_text)
          target_text='\t'+target_text+'\n'
          ##print(target_text)
          target_texts.append(target_text)
          for char in input_text:
              if char not in input_characters:
                  input_characters.add(char)
              
          for char in target_text:
              if char not in target_characters:
                  target_characters.add(char)
        
     
except Exception as e:
  print(e)
  traceback.print_exc()

len(input_texts)

len(target_texts)

## lets do some more preprocesing on input and target text

input_characters=sorted(list(input_characters))
target_characters=sorted(list(target_characters))

num_encoder_tokens=len(input_characters)
num_decoder_tokens=len(target_characters)

max_encoder_seq_length=max([ len(i) for i in input_texts])
max_decoder_seq_length=max([ len(i) for i in target_texts])


print("Number of  encoded tokens are ",len(input_characters))
print("Number of  decoded tokens are ",len(target_characters))

print("Maximum length of sequence in Input sequences is",max_encoder_seq_length)
print("Maximum length of sequence in Output sequences is", max_decoder_seq_length)

## create a dictionary set for every sententces word and it index
input_word_index=dict([(char,i) for i,char in enumerate(input_characters)])
target_word_index=dict([(char,i) for i,char in enumerate(target_characters)])

## Now we need 3 types of data which includes like input data ,target-data and target state from encoder

## so lets create it

encoder_input_data=np.zeros((len(input_texts), max_encoder_seq_length, num_encoder_tokens),dtype='float32')
decoder_input_data=np.zeros((len(input_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')
decoder_target_data=np.zeros((len(target_texts), max_decoder_seq_length, num_decoder_tokens),dtype='float32')

print(encoder_input_data.shape)
print(decoder_input_data.shape)
print(decoder_target_data.shape)

for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):
  for t, char in enumerate(input_text):
    encoder_input_data[i, t, input_word_index[char]] = 1.
  for t, char in enumerate(target_text):
    # decoder_target_data is ahead of decoder_input_data by one timestep
    decoder_input_data[i, t, target_word_index[char]] = 1.
    if t > 0:
      # decoder_target_data will be ahead by one timestep
      # and will not include the start character.
      decoder_target_data[i, t - 1, target_word_index[char]] = 1.

## define some basic configs
batch_size = 64  # batch size for training
epochs = 100  # number of epochs to train for
latent_dim = 256  # latent dimensionality of the encoding space

##Define Encoder Architecture

encoder_input=Input(shape=(None,num_encoder_tokens))
encoder=LSTM(latent_dim,return_state=True)
_, state_h, state_c = encoder(encoder_input)
encoder_states = [state_h, state_c]

##Not lets Define Decoder Architecture

Decoder_input=Input(shape=(None,num_decoder_tokens))
decoder=LSTM(latent_dim,return_sequences=True,return_state=True)
decoder_outputs,_,_ = decoder(Decoder_input,initial_state=encoder_states)
decoder_dense = Dense(num_decoder_tokens, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

# model=Model(inputs=[encoder_input,Decoder_input],outputs=decoder_outputs)

# model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc'])
# model.summary()

# model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
#           batch_size=batch_size,
#           epochs=200,
#           validation_split=0.2)

# model.save('simpleparaphrasing.h5')



model = load_model('simpleparaphrasing.h5')

encoder_model = Model(encoder_input, encoder_states)

decoder_state_input_h = Input(shape=(latent_dim,))
decoder_state_input_c = Input(shape=(latent_dim,))
decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]

decoder_outputs, state_h, state_c = decoder(
  Decoder_input, initial_state=decoder_states_inputs)
decoder_states = [state_h, state_c]
decoder_outputs = decoder_dense(decoder_outputs)

decoder_model = Model(
  [Decoder_input] + decoder_states_inputs,
  [decoder_outputs] + decoder_states)

  # reverse-lookup token index to turn sequences back to characters
reverse_input_char_index = dict(
  (i, char) for char, i in input_word_index.items())
reverse_target_char_index = dict(
  (i, char) for char, i in target_word_index.items())

def decode_sequence(input_seq):
      # encode the input sequence to get the internal state vectors.
  states_value = encoder_model.predict(input_seq)
  
  # generate empty target sequence of length 1 with only the start character
  target_seq = np.zeros((1, 1, num_decoder_tokens))
  target_seq[0, 0, target_word_index['\t']] = 1.
  
  # output sequence loop
  stop_condition = False
  decoded_sentence = ''
  while not stop_condition:
    output_tokens, h, c = decoder_model.predict(
      [target_seq] + states_value)
    
    # sample a token and add the corresponding character to the 
    # decoded sequence
    sampled_token_index = np.argmax(output_tokens[0, -1, :])
    sampled_char = reverse_target_char_index[sampled_token_index]
    decoded_sentence += sampled_char
    
    # check for the exit condition: either hitting max length
    # or predicting the 'stop' character
    if (sampled_char == '\n' or 
        len(decoded_sentence) > max_decoder_seq_length):
      stop_condition = True
      
    # update the target sequence (length 1).
    target_seq = np.zeros((1, 1, num_decoder_tokens))
    target_seq[0, 0, sampled_token_index] = 1.
    
    # update states
    states_value = [h, c]
    
  return decoded_sentence

