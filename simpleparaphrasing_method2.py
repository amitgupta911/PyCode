# -*- coding: utf-8 -*-
"""SimpleParaphrasing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C7-9kdkHQiTyS-cg1NajPN8dTV9GWyCj
"""

## import libraries 
from keras.layers import Dense,LSTM,Embedding,GRU,Input,Dropout
from keras.models import Sequential,Model
from keras.preprocessing.text import Tokenizer
import numpy as np
import keras.utils as KU
import traceback
import numpy as np
from keras.preprocessing.sequence import pad_sequences


# run = wandb.init()
# config = run.config
# config.optimizer = "adam"
# config.epochs = 10
# config.hidden_nodes = 100

##read dataset
with open('quora_duplicate_questions.tsv',encoding="utf-8") as f:
  data=f.read().split('\n')

data[1].split('\t')[5]

corpus=[]
for i in data:
  line=i.split('\t')
  if(i!= " " and len(line)>5):
    is_duplicate=line[5]
    if(is_duplicate=='1'):
        corpus.append(i)

corpus

len(corpus)

input_texts=[]
target_texts=[]
input_characters=set()
target_characters=set()

## Not create input text as English and output text as Hindi
try:
  for line in corpus[:10000]:
      ##print(line)
      if(line !=""):
          text=line.split('\t')
          input_text=text[3]
          target_text=text[4]
          input_texts.append(input_text.lower())
          target_text='\t'+target_text.lower()+'\n'
          ##print(target_text)
          target_texts.append(target_text)
          for char in input_text:
              if char not in input_characters:
                  input_characters.add(char)
              
          for char in target_text:
              if char not in target_characters:
                  target_characters.add(char)
except Exception as e:
  print(e)
  traceback.print_exc()

max_sequence_len_input = max([len(x) for x in input_texts])
max_sequence_len_target = max([len(x) for x in target_texts])

input_tokenizer=Tokenizer()
input_tokenizer.fit_on_texts(input_text)

target_tokenizer=Tokenizer()
target_tokenizer.fit_on_texts(target_text)

input_vocab_size=input_tokenizer.word_index+1
target_vocab_size=target_tokenizer.word_index+1

## Now Generate the embeddings from Glove vector
embeddings_index={}
def Generate_Embeddings():
    f=open('../content/glove.6B.100d.txt') 
    for line in f:
        print(line)
        values=line.split()
        word=values[0]
        coefs=np.array(values[1:],dtype="float32")
        embeddings_index[word]=coefs
    f.close()

## Now Generate Metrix of embedding as per our data
embedding_matrix = np.zeros((input_vocab_size, 100))
def Generate_Embeddings_Matrix():
    for word, i in input_tokenizer.word_index.items():
        print(word,":::",i)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

# encode and pad sequences
def encode_sequences(tokenizer, length, lines):
    # integer encode sequences
    seq = tokenizer.texts_to_sequences(lines)
    # pad sequences with 0 values
    seq = pad_sequences(seq, maxlen=length, padding='post')
    return seq

Generate_Embeddings()

encoder_input_data=encode_sequences(input_tokenizer,max_sequence_len_input,input_texts)
decoder_input_data=encode_sequences(target_tokenizer,max_sequence_len_target,target_texts)
model=Sequential()
model.add(Embedding(input_vocab_size,100, weights=[embedding_matrix], input_length=max_sequence_len_input-1, trainable=False))

##Define Encoder Architecture

encoder_input=Input(shape=(None,input_vocab_size))
encoder=LSTM(512,return_state=True)
model.add(Dropout(0.3))
_, state_h, state_c = encoder(encoder_input)
encoder_states = [state_h, state_c]

##Not lets Define Decoder Architecture

Decoder_input=Input(shape=(None,target_vocab_size))
decoder=LSTM(512,return_sequences=True,return_state=True)
model.add(Dropout(0.3))
decoder_outputs,_,_ = decoder(Decoder_input,initial_state=encoder_states)
decoder_dense = Dense(target_vocab_size, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)

model=Model(inputs=[encoder_input,Decoder_input],outputs=decoder_outputs)

model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc'])
model.summary()

model.fit(encoder_input_data, decoder_input_data,
          batch_size=512,
          epochs=20,
          validation_split=0.2)

model.save('simpleparaphrasing1.h5')

