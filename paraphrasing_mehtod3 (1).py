# -*- coding: utf-8 -*-
"""Paraphrasing-Mehtod3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ayVBTKVAx6paur5wJEvyWCfJ5dRg8rdc
"""

## import libraries 
from keras.layers import Dense,LSTM,Embedding,GRU,Input,Dropout,RepeatVector,Flatten
from keras.models import Sequential,Model
from keras.preprocessing.text import Tokenizer
import numpy as np
import keras.utils as KU
import traceback
import numpy as np
from keras.preprocessing.sequence import pad_sequences
from keras import optimizers
import tensorflow as tf
import keras.losses as KL
from keras.callbacks import ModelCheckpoint,EarlyStopping,TensorBoard

num_words=1000

##read dataset
with open('quora_duplicate_questions.tsv',encoding="utf-8") as f:
  data=f.read().split('\n')

corpus=[]
for i in data:
  line=i.split('\t')
  if(i!= " " and len(line)>5):
    is_duplicate=line[5]
    if(is_duplicate=='1'):
        corpus.append(i)

input_texts=[]
target_texts=[]
input_characters=set()
target_characters=set()

## Not create input text as English and output text as Hindi
try:
  for line in corpus[:1000]:
      ##print(line)
      if(line !=""):
          text=line.split('\t')
          input_text=text[3]
          target_text=text[4]
          input_texts.append(input_text.lower())
          target_text='ssss '+target_text.lower()+' eeee'
          ##print(target_text)
          target_texts.append(target_text)
          for char in input_text:
              if char not in input_characters:
                  input_characters.add(char)
              
          for char in target_text:
              if char not in target_characters:
                  target_characters.add(char)
except Exception as e:
  print(e)
  traceback.print_exc()

max_sequence_len_input = max([len(x) for x in input_texts])
max_sequence_len_target = max([len(x) for x in target_texts])

input_tokenizer=Tokenizer(filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n')
input_tokenizer.fit_on_texts(input_texts)

target_tokenizer=Tokenizer(filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n')
target_tokenizer.fit_on_texts(target_texts)

input_vocab_size=len(input_tokenizer.word_index)+1
target_vocab_size=len(target_tokenizer.word_index)+1

## Now Generate the embeddings from Glove vector
embeddings_index={}
def Generate_Embeddings():
    f=open('../content/glove.6B.100d.txt') 
    for line in f:
        #print(line)
        values=line.split()
        word=values[0]
        coefs=np.array(values[1:],dtype="float32")
        embeddings_index[word]=coefs
    f.close()

## Now Generate Metrix of embedding as per our data
embedding_matrix = np.zeros((input_vocab_size, 100))
def Generate_Embeddings_Matrix():
    for word, i in input_tokenizer.word_index.items():
        #print(word,":::",i)
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

Generate_Embeddings()

# encode and pad sequences
def encode_sequences(tokenizer, length, lines):
    # integer encode sequences
    seq = tokenizer.texts_to_sequences(lines)
    # pad sequences with 0 values
    seq = pad_sequences(seq, maxlen=length, padding='post')
    return seq

encoder_input_data=encode_sequences(input_tokenizer,max_sequence_len_input,input_texts)
decoder_input_data=encode_sequences(target_tokenizer,max_sequence_len_target,target_texts)
decoder_input_data_final=decoder_input_data[:,:-1]
decoder_target_data=decoder_input_data[:,1:]

## create an encoder archetecture
encoder_input=Input(shape=(None,))
#encoder_embeddings=Embedding(input_vocab_size, 100, input_length=max_sequence_len_input-1, mask_zero=True,trainable=False)
encoder_embeddings=Embedding(input_dim=input_vocab_size,output_dim=128,name='encoder_embedding')
encoder_gru_1=GRU(512,name="encoder_gru_1",return_sequences=True)
encoder_gru_2=GRU(512,name="encoder_gru_2",return_sequences=True)
encoder_gru_3=GRU(512,name="encoder_gru_3",return_sequences=False)

def Create_Encoder():
  #Here we will use functional api of keras
  net=encoder_input
  net=encoder_embeddings(net)
  ## not need to connect GRU units
  net=encoder_gru_1(net)
  net=encoder_gru_2(net)
  net=encoder_gru_3(net)
  
  return net

##Now create decoder architecture
decoder_initial_state=Input(shape=(512,),name="decoder_initial_state")
decoder_input=Input(shape=(None,),name="decoder_input")
#decoder_embeddings=Embedding(target_vocab_size, 100, input_length=max_sequence_len_target-1,trainable=False)
decoder_embeddings=Embedding(input_dim=target_vocab_size,output_dim=128,name='decoder_embedding')
decoder_gru_1=GRU(512,name="decoder_gru_1",return_sequences=True)
decoder_gru_2=GRU(512,name="decoder_gru_2",return_sequences=True)
decoder_gru_3=GRU(512,name="decoder_gru_3",return_sequences=True)
decoder_flatten=Flatten()
decoder_dense=Dense(target_vocab_size,activation="linear")

def Create_Decoder(initial_state):
  #Here we will use functional api of keras
  net=decoder_input
  net=decoder_embeddings(net)
  ## not need to connect GRU units
  net=decoder_gru_1(net,initial_state=initial_state)
  net=decoder_gru_2(net,initial_state=initial_state)
  net=decoder_gru_3(net,initial_state=initial_state)
  #net=decoder_flatten(net)
  net=decoder_dense(net)
  
  return net

##Create a Training Model
encoder_output=Create_Encoder()
decoder_output=Create_Decoder(encoder_output)
model_train=Model(inputs=[encoder_input,decoder_input],outputs=[decoder_output])

## create an Encoder Model
model_encoder=Model(inputs=[encoder_input],outputs=[encoder_output])

## create a decoder model
decoder_output=Create_Decoder(decoder_initial_state)
model_decoder=Model(inputs=[decoder_input,decoder_initial_state],outputs=[decoder_output])

optimizer=optimizers.RMSprop(lr=1e-3)

def sparse_cross_entropy(y_true, y_pred):
    """
    Calculate the cross-entropy loss between y_true and y_pred.
    
    y_true is a 2-rank tensor with the desired output.
    The shape is [batch_size, sequence_length] and it
    contains sequences of integer-tokens.

    y_pred is the decoder's output which is a 3-rank tensor
    with shape [batch_size, sequence_length, num_words]
    so that for each sequence in the batch there is a one-hot
    encoded array of length num_words.
    """

    # Calculate the loss. This outputs a
    # 2-rank tensor of shape [batch_size, sequence_length]
    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true,
                                                          logits=y_pred)

    # Keras may reduce this across the first axis (the batch)
    # but the semantics are unclear, so to be sure we use
    # the loss across the entire 2-rank tensor, we reduce it
    # to a single scalar with the mean function.
    loss_mean = tf.reduce_mean(loss)

    return loss_mean

decoder_target = tf.placeholder(dtype='float32', shape=(None, None))

model_train.compile(optimizer=optimizer, loss=KL.sparse_categorical_crossentropy,metrics=['acc'],target_tensors=[decoder_target])
model_train.summary()

path_checkpoint='21_checkpoint.keras'
callback_checkpoint=ModelCheckpoint(filepath=path_checkpoint,monitor='val_loss',verbose=1, save_best_only=True, save_weights_only=True)
callback_earlystopping=EarlyStopping(monitor='val_loss',verbose=1,patience=3)
callback_tensorboard=TensorBoard(log_dir='../content/21_logs/',histogram_freq=0,write_graph=False)
callbacks=[callback_checkpoint,callback_earlystopping,callback_tensorboard]

x_data={
    'encoder-input':encoder_input_data,
    'decoder-input':decoder_input_data
}
y_data={
    'decoder_output':decoder_target_data
}

model_train.fit([encoder_input_data, decoder_input_data],decoder_target_data.reshape(decoder_target_data.shape[0], decoder_target_data.shape[1], 1),
          batch_size=640,
          epochs=20,
          validation_split=0.3,callbacks=callbacks)

model_train.save('ModelNew.h5')

decoder_target_data.shape

